{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./frankenstein_env/lib/python3.10/site-packages (4.48.2)\n",
      "Requirement already satisfied: datasets in ./frankenstein_env/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in ./frankenstein_env/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in ./frankenstein_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./frankenstein_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./frankenstein_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: aiohttp in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: xxhash in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./frankenstein_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: jinja2 in ./frankenstein_env/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: networkx in ./frankenstein_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./frankenstein_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./frankenstein_env/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./frankenstein_env/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./frankenstein_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./frankenstein_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./frankenstein_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./frankenstein_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in ./frankenstein_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./frankenstein_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./frankenstein_env/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./frankenstein_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./frankenstein_env/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./frankenstein_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the '/Users/prasiddhapradhan/Desktop/FrankensteinCPUStarter/frankenstein_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch pandas numpy\n",
    "%pip install transformers[torch]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 481 entries, 0 to 480\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    481 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ï»¿The Project Gutenberg eBook of Frankenstein; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Further corrections by Menno de Leeuw.\\n\\n\\n**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am already far north of London, and as I wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its productions and features may be without ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But supposing all these conjectures to be fals...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ï»¿The Project Gutenberg eBook of Frankenstein; ...\n",
       "1  Further corrections by Menno de Leeuw.\\n\\n\\n**...\n",
       "2  I am already far north of London, and as I wal...\n",
       "3  Its productions and features may be without ex...\n",
       "4  But supposing all these conjectures to be fals..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"frankenstein_chunks.csv\")\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 CLEAN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Further corrections by Menno de Leeuw.\\n\\n\\n**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am already far north of London, and as I wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Its productions and features may be without ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But supposing all these conjectures to be fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You may remember that a\\nhistory of all the vo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Further corrections by Menno de Leeuw.\\n\\n\\n**...\n",
       "1  I am already far north of London, and as I wal...\n",
       "2  Its productions and features may be without ex...\n",
       "3  But supposing all these conjectures to be fals...\n",
       "4  You may remember that a\\nhistory of all the vo..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['text'].str.contains(\"Project Gutenberg\") == False]\n",
    "\n",
    "#Remove empty spaces and reset index\n",
    "df['text'] = df['text'].str.strip()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#check the cleaned dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2 CONVERT DATA INTO HUGGING FACE DATASET FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 464\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "#Convert pandas dataframe to hugging face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP3: TOKENIZE THE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9e0d09c3c44357be38abf7e79b65d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Further corrections by Menno de Leeuw.\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\\n\\n\\n\\n\\nFrankenstein;\\n\\nor, the Modern Prometheus\\n\\nby Mary Wollstonecraft (Godwin) Shelley\\n\\n\\n CONTENTS\\n\\n Letter 1\\n Letter 2\\n Letter 3\\n Letter 4\\n Chapter 1\\n Chapter 2\\n Chapter 3\\n Chapter 4\\n Chapter 5\\n Chapter 6\\n Chapter 7\\n Chapter 8\\n Chapter 9\\n Chapter 10\\n Chapter 11\\n Chapter 12\\n Chapter 13\\n Chapter 14\\n Chapter 15\\n Chapter 16\\n Chapter 17\\n Chapter 18\\n Chapter 19\\n Chapter 20\\n Chapter 21\\n Chapter 22\\n Chapter 23\\n Chapter 24\\n\\n\\n\\n\\nLetter 1\\n\\n_To Mrs. Saville, England._\\n\\n\\nSt. Petersburgh, Dec. 11th, 17â€”.\\n\\n\\nYou will rejoice to hear that no disaster has accompanied the\\ncommencement of an enterprise which you have regarded with such evil\\nforebodings. I arrived here yesterday, and my first task is to assure\\nmy dear sister of my welfare and increasing confidence in the success\\nof my undertaking.', 'input_ids': [13518, 26251, 416, 337, 1697, 78, 390, 5741, 84, 86, 13, 628, 198, 8162, 33303, 3963, 3336, 21965, 23680, 402, 3843, 1677, 13246, 38, 412, 39453, 8782, 15154, 1677, 30516, 1268, 26, 6375, 11, 3336, 19164, 28778, 4810, 2662, 2767, 13909, 2937, 17202, 628, 628, 198, 17439, 37975, 26, 198, 198, 273, 11, 262, 12495, 42696, 198, 198, 1525, 5335, 370, 692, 6440, 3323, 357, 13482, 5404, 8, 46854, 628, 198, 22904, 15365, 628, 18121, 352, 198, 18121, 362, 198, 18121, 513, 198, 18121, 604, 198, 7006, 352, 198, 7006, 362, 198, 7006, 513, 198, 7006, 604, 198, 7006, 642, 198, 7006, 718, 198, 7006, 767, 198, 7006, 807, 198, 7006, 860, 198, 7006, 838, 198, 7006, 1367, 198, 7006, 1105, 198, 7006, 1511, 198, 7006, 1478, 198, 7006, 1315, 198, 7006, 1467, 198, 7006, 1596, 198, 7006, 1248, 198, 7006, 678, 198, 7006, 1160, 198, 7006, 2310, 198, 7006, 2534, 198, 7006, 2242, 198, 7006, 1987, 628, 628, 198, 45708, 352, 198, 198, 62, 2514, 9074, 13, 8858, 8270, 11, 4492, 13557, 628, 198, 1273, 13, 15722, 9228, 11, 4280, 13, 1367, 400, 11, 1596, 960, 13, 628, 198, 1639, 481, 46201, 284, 3285, 326, 645, 9336, 468, 11791, 262, 198, 9503, 594, 434, 286, 281, 13953, 543, 345, 423, 11987, 351, 884, 6181, 198, 754, 65, 375, 654, 13, 314, 5284, 994, 7415, 11, 290, 616, 717, 4876, 318, 284, 19832, 198, 1820, 13674, 6621, 286, 616, 9490, 290, 3649, 6628, 287, 262, 1943, 198, 1659, 616, 25971, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load DistilGPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Set padding token (GPT-2 models donâ€™t have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check tokenized output\n",
    "print(tokenized_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP4: PREPARE FOR TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "#Data collator for MLM(Masked Language Modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP5: LOAD DISTILGPT-2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "#Load pre-trained distilGPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP6: DEFINE TRAINING PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prasiddhapradhan/Desktop/FrankensteinCPUStarter/frankenstein_env/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/ps/kr50jcsx05ngv6vznhvghhp00000gn/T/ipykernel_61420/691934396.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./frankenstein_model\",  # Save model here\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,  # Adjust if needed\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # Normally a separate validation set\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP7: START TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 04:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.997200</td>\n",
       "      <td>3.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.726200</td>\n",
       "      <td>3.450395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.607400</td>\n",
       "      <td>3.405510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=348, training_loss=3.7520053468901535, metrics={'train_runtime': 282.6821, 'train_samples_per_second': 4.924, 'train_steps_per_second': 1.231, 'total_flos': 181862538412032.0, 'train_loss': 3.7520053468901535, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./frankenstein_model/tokenizer_config.json',\n",
       " './frankenstein_model/special_tokens_map.json',\n",
       " './frankenstein_model/vocab.json',\n",
       " './frankenstein_model/merges.txt',\n",
       " './frankenstein_model/added_tokens.json',\n",
       " './frankenstein_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./frankenstein_model\")\n",
    "tokenizer.save_pretrained(\"./frankenstein_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./frankenstein_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./frankenstein_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP8: EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 27.87530429372864\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load trained model\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\"./frankenstein_model\")\n",
    "\n",
    "# Load tokenizer again\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Define evaluation function\n",
    "def compute_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = trained_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    return math.exp(loss)\n",
    "\n",
    "# Example evaluation\n",
    "sample_text = \"It was on a dreary night of November that I beheld the accomplishment of my toils.\"\n",
    "perplexity = compute_perplexity(sample_text)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP9: GENERATE FRANKENSTEIN-STYLE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the storm raged outside, the creature whispered to me the words of my father during my speech of gratitude.\n",
      "\n",
      "â€œWhy do you weep? Why do you lose sight of it?\n",
      "My father exclaimed: â€˜We must not have loved you more; we are so dear to you. But let me be with you, the old soul.â€™\n",
      "\n",
      "â€œThen my father breathed as if it had long been so cruel, and if youâ€™re not a stranger\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"As the storm raged outside, the creature whispered\"\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frankenstein_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
